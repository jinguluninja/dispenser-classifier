{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(135109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sensor = '72'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    X = []\n",
    "    y = []\n",
    "    data_dir_0 = sorted(os.listdir(os.path.join('pac_data', sensor, '0')))   \n",
    "    i = 0\n",
    "    for f in data_dir_0:\n",
    "        if i % 100 == 0: print(i)\n",
    "        i += 1\n",
    "        X.append(np.load(os.path.join('pac_data', sensor, '0', f))['x'])\n",
    "        y.append(0)\n",
    "    X = np.asarray(X, dtype=np.float16)\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X[indices[:int(0.8*len(indices))]]\n",
    "    y_train = y[indices[:int(0.8*len(indices))]]\n",
    "    X_val = X[indices[int(0.8*len(indices)):]]\n",
    "    y_val = y[indices[int(0.8*len(indices)):]]\n",
    "    labels_present = os.listdir(os.path.join('pac_data', sensor))\n",
    "    if '1' in labels_present:\n",
    "        X = []\n",
    "        y = []\n",
    "        data_dir_1 = sorted(os.listdir(os.path.join('pac_data', sensor, '1')))\n",
    "        i = 0\n",
    "        for f in data_dir_1:\n",
    "            if i % 100 == 0: print(i)\n",
    "            i += 1\n",
    "            X.append(np.load(os.path.join('pac_data', sensor, '1', f))['x'])\n",
    "            y.append(1)        \n",
    "        X = np.asarray(X, dtype=np.float16)\n",
    "        y = np.asarray(y, dtype=int)\n",
    "        indices = np.arange(y.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = np.concatenate([X_train, X[indices[:int(0.8*len(indices))]]])\n",
    "        y_train = np.concatenate([y_train, y[indices[:int(0.8*len(indices))]]])\n",
    "        X_val = np.concatenate([X_val, X[indices[int(0.8*len(indices)):]]])\n",
    "        y_val = np.concatenate([y_val, y[indices[int(0.8*len(indices)):]]])\n",
    "    X_train = np.expand_dims(np.asarray(X_train, dtype=np.float16), axis=3)\n",
    "    y_train = np.asarray(y_train, dtype=int)\n",
    "    X_val = np.expand_dims(np.asarray(X_val, dtype=np.float16), axis=3)\n",
    "    y_val = np.asarray(y_val, dtype=int)\n",
    "    X_train /= 5.\n",
    "    X_val /= 5.\n",
    "    mean_arr = np.mean(X_train, axis = 0)\n",
    "    np.save(os.path.join('pac_data', sensor, 'mean.npy'), mean_arr)\n",
    "    X_train -= mean_arr \n",
    "    X_val -= mean_arr  \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4859, 240, 320, 1) (4859,) (1215, 240, 320, 1) (1215,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling2D, Activation, Dense, Input\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers import concatenate\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import conv_utils\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras.applications.xception import Xception\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#linear\n",
    "def linear(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, x, name='linear')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef xception(dropout):\\n    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\\n    x = base_model.output\\n    x = Flatten()(x)\\n    x = Dropout(dropout)(x) # drop prob\\n    predictions = Dense(2, activation='softmax')(x)\\n    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\\n    return model\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import SeparableConv2D\n",
    "\n",
    "def xception(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))    \n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2), use_bias=False, name='block1_conv1')(inputs)\n",
    "    x = BatchNormalization(name='block1_conv1_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv1_act')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', use_bias=False, name='block1_conv2')(x)\n",
    "    x = BatchNormalization(name='block1_conv2_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv2_act')(x)\n",
    "\n",
    "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block2_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(256, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block3_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block3_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(728, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block4_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block4_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    for i in range(8):\n",
    "        residual = x\n",
    "        prefix = 'block' + str(i + 5)\n",
    "\n",
    "        x = Activation('relu', name=prefix + '_sepconv1_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv1_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv2_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv2_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv3_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv3_bn')(x)\n",
    "\n",
    "        x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(1024, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block13_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block13_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block13_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv1_act')(x)\n",
    "\n",
    "    x = SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv2_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv2_act')(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "def xception(dropout):\n",
    "    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x) # drop prob\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vanillacnn(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1)) \n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 240, 320, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 120, 160, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 120, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 120, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 40, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 10, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 10, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 10, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 390,786\n",
      "Trainable params: 389,826\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model' + sensor + '.h5'\n",
    "model = vanillacnn(0.5)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "stop = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, stop]\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4859 samples, validate on 1215 samples\n",
      "Epoch 1/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.7474 - sparse_categorical_accuracy: 0.6983Epoch 00000: val_loss improved from inf to 0.78872, saving model to model72.h5\n",
      "4859/4859 [==============================] - 16s - loss: 0.7464 - sparse_categorical_accuracy: 0.6983 - val_loss: 0.7887 - val_sparse_categorical_accuracy: 0.7144\n",
      "Epoch 2/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.4483 - sparse_categorical_accuracy: 0.8065Epoch 00001: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.4473 - sparse_categorical_accuracy: 0.8068 - val_loss: 0.9071 - val_sparse_categorical_accuracy: 0.7144\n",
      "Epoch 3/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.3579 - sparse_categorical_accuracy: 0.8421Epoch 00002: val_loss improved from 0.78872 to 0.75480, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.3592 - sparse_categorical_accuracy: 0.8415 - val_loss: 0.7548 - val_sparse_categorical_accuracy: 0.7144\n",
      "Epoch 4/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.3117 - sparse_categorical_accuracy: 0.8640Epoch 00003: val_loss improved from 0.75480 to 0.44346, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.3116 - sparse_categorical_accuracy: 0.8638 - val_loss: 0.4435 - val_sparse_categorical_accuracy: 0.7193\n",
      "Epoch 5/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.2883 - sparse_categorical_accuracy: 0.8781Epoch 00004: val_loss improved from 0.44346 to 0.23842, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.2883 - sparse_categorical_accuracy: 0.8784 - val_loss: 0.2384 - val_sparse_categorical_accuracy: 0.9062\n",
      "Epoch 6/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.2635 - sparse_categorical_accuracy: 0.8882Epoch 00005: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.2638 - sparse_categorical_accuracy: 0.8880 - val_loss: 0.2665 - val_sparse_categorical_accuracy: 0.8691\n",
      "Epoch 7/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.2472 - sparse_categorical_accuracy: 0.8980Epoch 00006: val_loss improved from 0.23842 to 0.19670, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.2464 - sparse_categorical_accuracy: 0.8983 - val_loss: 0.1967 - val_sparse_categorical_accuracy: 0.9276\n",
      "Epoch 8/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.2243 - sparse_categorical_accuracy: 0.9079Epoch 00007: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.2252 - sparse_categorical_accuracy: 0.9080 - val_loss: 0.2112 - val_sparse_categorical_accuracy: 0.9152\n",
      "Epoch 9/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.2222 - sparse_categorical_accuracy: 0.9114Epoch 00008: val_loss improved from 0.19670 to 0.17599, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.2218 - sparse_categorical_accuracy: 0.9117 - val_loss: 0.1760 - val_sparse_categorical_accuracy: 0.9292\n",
      "Epoch 10/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.2021 - sparse_categorical_accuracy: 0.9201Epoch 00009: val_loss improved from 0.17599 to 0.16799, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.2020 - sparse_categorical_accuracy: 0.9201 - val_loss: 0.1680 - val_sparse_categorical_accuracy: 0.9325\n",
      "Epoch 11/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1949 - sparse_categorical_accuracy: 0.9236Epoch 00010: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1949 - sparse_categorical_accuracy: 0.9236 - val_loss: 0.1780 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 12/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1895 - sparse_categorical_accuracy: 0.9259Epoch 00011: val_loss improved from 0.16799 to 0.16450, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1890 - sparse_categorical_accuracy: 0.9261 - val_loss: 0.1645 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 13/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1895 - sparse_categorical_accuracy: 0.9253Epoch 00012: val_loss improved from 0.16450 to 0.15259, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1890 - sparse_categorical_accuracy: 0.9255 - val_loss: 0.1526 - val_sparse_categorical_accuracy: 0.9358\n",
      "Epoch 14/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1744 - sparse_categorical_accuracy: 0.9280Epoch 00013: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1740 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.1647 - val_sparse_categorical_accuracy: 0.9317\n",
      "Epoch 15/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1721 - sparse_categorical_accuracy: 0.9323Epoch 00014: val_loss improved from 0.15259 to 0.14282, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1724 - sparse_categorical_accuracy: 0.9323 - val_loss: 0.1428 - val_sparse_categorical_accuracy: 0.9366\n",
      "Epoch 16/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1708 - sparse_categorical_accuracy: 0.9356Epoch 00015: val_loss improved from 0.14282 to 0.13892, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1706 - sparse_categorical_accuracy: 0.9358 - val_loss: 0.1389 - val_sparse_categorical_accuracy: 0.9407\n",
      "Epoch 17/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1662 - sparse_categorical_accuracy: 0.9363Epoch 00016: val_loss improved from 0.13892 to 0.13510, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1654 - sparse_categorical_accuracy: 0.9366 - val_loss: 0.1351 - val_sparse_categorical_accuracy: 0.9416\n",
      "Epoch 18/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1591 - sparse_categorical_accuracy: 0.9356Epoch 00017: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1594 - sparse_categorical_accuracy: 0.9356 - val_loss: 0.1370 - val_sparse_categorical_accuracy: 0.9457\n",
      "Epoch 19/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1514 - sparse_categorical_accuracy: 0.9416Epoch 00018: val_loss improved from 0.13510 to 0.13309, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1512 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.1331 - val_sparse_categorical_accuracy: 0.9432\n",
      "Epoch 20/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1532 - sparse_categorical_accuracy: 0.9373Epoch 00019: val_loss improved from 0.13309 to 0.13118, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1532 - sparse_categorical_accuracy: 0.9372 - val_loss: 0.1312 - val_sparse_categorical_accuracy: 0.9424\n",
      "Epoch 21/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1567 - sparse_categorical_accuracy: 0.9412Epoch 00020: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1566 - sparse_categorical_accuracy: 0.9413 - val_loss: 0.1417 - val_sparse_categorical_accuracy: 0.9424\n",
      "Epoch 22/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1506 - sparse_categorical_accuracy: 0.9389Epoch 00021: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1508 - sparse_categorical_accuracy: 0.9391 - val_loss: 0.1426 - val_sparse_categorical_accuracy: 0.9424\n",
      "Epoch 23/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1348 - sparse_categorical_accuracy: 0.9445Epoch 00022: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1361 - sparse_categorical_accuracy: 0.9442 - val_loss: 0.1340 - val_sparse_categorical_accuracy: 0.9457\n",
      "Epoch 24/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1405 - sparse_categorical_accuracy: 0.9462Epoch 00023: val_loss improved from 0.13118 to 0.12127, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1409 - sparse_categorical_accuracy: 0.9459 - val_loss: 0.1213 - val_sparse_categorical_accuracy: 0.9465\n",
      "Epoch 25/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1406 - sparse_categorical_accuracy: 0.9437Epoch 00024: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1400 - sparse_categorical_accuracy: 0.9440 - val_loss: 0.1250 - val_sparse_categorical_accuracy: 0.9506\n",
      "Epoch 26/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1338 - sparse_categorical_accuracy: 0.9483Epoch 00025: val_loss improved from 0.12127 to 0.11970, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1335 - sparse_categorical_accuracy: 0.9483 - val_loss: 0.1197 - val_sparse_categorical_accuracy: 0.9490\n",
      "Epoch 27/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1293 - sparse_categorical_accuracy: 0.9514Epoch 00026: val_loss improved from 0.11970 to 0.11805, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1290 - sparse_categorical_accuracy: 0.9514 - val_loss: 0.1181 - val_sparse_categorical_accuracy: 0.9523\n",
      "Epoch 28/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1302 - sparse_categorical_accuracy: 0.9491Epoch 00027: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1303 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.1291 - val_sparse_categorical_accuracy: 0.9440\n",
      "Epoch 29/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1305 - sparse_categorical_accuracy: 0.9483Epoch 00028: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1304 - sparse_categorical_accuracy: 0.9481 - val_loss: 0.1212 - val_sparse_categorical_accuracy: 0.9498\n",
      "Epoch 30/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1228 - sparse_categorical_accuracy: 0.9499Epoch 00029: val_loss improved from 0.11805 to 0.11681, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1225 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.1168 - val_sparse_categorical_accuracy: 0.9473\n",
      "Epoch 31/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1197 - sparse_categorical_accuracy: 0.9510Epoch 00030: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1192 - sparse_categorical_accuracy: 0.9512 - val_loss: 0.1241 - val_sparse_categorical_accuracy: 0.9531\n",
      "Epoch 32/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1211 - sparse_categorical_accuracy: 0.9516Epoch 00031: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1205 - sparse_categorical_accuracy: 0.9518 - val_loss: 0.1279 - val_sparse_categorical_accuracy: 0.9473\n",
      "Epoch 33/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1207 - sparse_categorical_accuracy: 0.9514Epoch 00032: val_loss improved from 0.11681 to 0.11645, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1204 - sparse_categorical_accuracy: 0.9514 - val_loss: 0.1165 - val_sparse_categorical_accuracy: 0.9556\n",
      "Epoch 34/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1309 - sparse_categorical_accuracy: 0.9433Epoch 00033: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1316 - sparse_categorical_accuracy: 0.9432 - val_loss: 0.1466 - val_sparse_categorical_accuracy: 0.9465\n",
      "Epoch 35/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1155 - sparse_categorical_accuracy: 0.9512Epoch 00034: val_loss improved from 0.11645 to 0.11453, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1153 - sparse_categorical_accuracy: 0.9512 - val_loss: 0.1145 - val_sparse_categorical_accuracy: 0.9506\n",
      "Epoch 36/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1095 - sparse_categorical_accuracy: 0.9574Epoch 00035: val_loss improved from 0.11453 to 0.11257, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1094 - sparse_categorical_accuracy: 0.9574 - val_loss: 0.1126 - val_sparse_categorical_accuracy: 0.9514\n",
      "Epoch 37/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1226 - sparse_categorical_accuracy: 0.9526Epoch 00036: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1222 - sparse_categorical_accuracy: 0.9527 - val_loss: 0.1236 - val_sparse_categorical_accuracy: 0.9506\n",
      "Epoch 38/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1240 - sparse_categorical_accuracy: 0.9483Epoch 00037: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1235 - sparse_categorical_accuracy: 0.9485 - val_loss: 0.1146 - val_sparse_categorical_accuracy: 0.9523\n",
      "Epoch 39/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1139 - sparse_categorical_accuracy: 0.9536Epoch 00038: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1137 - sparse_categorical_accuracy: 0.9537 - val_loss: 0.1210 - val_sparse_categorical_accuracy: 0.9506\n",
      "Epoch 40/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1069 - sparse_categorical_accuracy: 0.9576Epoch 00039: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1078 - sparse_categorical_accuracy: 0.9574 - val_loss: 0.1137 - val_sparse_categorical_accuracy: 0.9523\n",
      "Epoch 41/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1038 - sparse_categorical_accuracy: 0.9580Epoch 00040: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1044 - sparse_categorical_accuracy: 0.9578 - val_loss: 0.1128 - val_sparse_categorical_accuracy: 0.9547\n",
      "Epoch 42/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1027 - sparse_categorical_accuracy: 0.9584Epoch 00041: val_loss improved from 0.11257 to 0.11132, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1022 - sparse_categorical_accuracy: 0.9586 - val_loss: 0.1113 - val_sparse_categorical_accuracy: 0.9588\n",
      "Epoch 43/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0980 - sparse_categorical_accuracy: 0.9615Epoch 00042: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0981 - sparse_categorical_accuracy: 0.9615 - val_loss: 0.1146 - val_sparse_categorical_accuracy: 0.9539\n",
      "Epoch 44/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1060 - sparse_categorical_accuracy: 0.9603Epoch 00043: val_loss improved from 0.11132 to 0.11040, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.1063 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.1104 - val_sparse_categorical_accuracy: 0.9556\n",
      "Epoch 45/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1050 - sparse_categorical_accuracy: 0.9584Epoch 00044: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1056 - sparse_categorical_accuracy: 0.9584 - val_loss: 0.1176 - val_sparse_categorical_accuracy: 0.9539\n",
      "Epoch 46/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1063 - sparse_categorical_accuracy: 0.9570Epoch 00045: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1061 - sparse_categorical_accuracy: 0.9572 - val_loss: 0.1266 - val_sparse_categorical_accuracy: 0.9514\n",
      "Epoch 47/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0978 - sparse_categorical_accuracy: 0.9619Epoch 00046: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0974 - sparse_categorical_accuracy: 0.9621 - val_loss: 0.1215 - val_sparse_categorical_accuracy: 0.9498\n",
      "Epoch 48/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0995 - sparse_categorical_accuracy: 0.9603Epoch 00047: val_loss improved from 0.11040 to 0.10776, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.0993 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.1078 - val_sparse_categorical_accuracy: 0.9605\n",
      "Epoch 49/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1010 - sparse_categorical_accuracy: 0.9580Epoch 00048: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1006 - sparse_categorical_accuracy: 0.9582 - val_loss: 0.1346 - val_sparse_categorical_accuracy: 0.9506\n",
      "Epoch 50/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.1007 - sparse_categorical_accuracy: 0.9601Epoch 00049: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.1008 - sparse_categorical_accuracy: 0.9599 - val_loss: 0.1082 - val_sparse_categorical_accuracy: 0.9588\n",
      "Epoch 51/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0951 - sparse_categorical_accuracy: 0.9636Epoch 00050: val_loss improved from 0.10776 to 0.10540, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.0947 - sparse_categorical_accuracy: 0.9638 - val_loss: 0.1054 - val_sparse_categorical_accuracy: 0.9580\n",
      "Epoch 52/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0944 - sparse_categorical_accuracy: 0.9634Epoch 00051: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0947 - sparse_categorical_accuracy: 0.9632 - val_loss: 0.1165 - val_sparse_categorical_accuracy: 0.9539\n",
      "Epoch 53/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0987 - sparse_categorical_accuracy: 0.9605Epoch 00052: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0986 - sparse_categorical_accuracy: 0.9605 - val_loss: 0.1179 - val_sparse_categorical_accuracy: 0.9539\n",
      "Epoch 54/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0962 - sparse_categorical_accuracy: 0.9619Epoch 00053: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0961 - sparse_categorical_accuracy: 0.9619 - val_loss: 0.1163 - val_sparse_categorical_accuracy: 0.9539\n",
      "Epoch 55/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0901 - sparse_categorical_accuracy: 0.9659Epoch 00054: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0901 - sparse_categorical_accuracy: 0.9658 - val_loss: 0.1165 - val_sparse_categorical_accuracy: 0.9547\n",
      "Epoch 56/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0989 - sparse_categorical_accuracy: 0.9588Epoch 00055: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0987 - sparse_categorical_accuracy: 0.9588 - val_loss: 0.1116 - val_sparse_categorical_accuracy: 0.9547\n",
      "Epoch 57/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0857 - sparse_categorical_accuracy: 0.9642Epoch 00056: val_loss improved from 0.10540 to 0.10329, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.0854 - sparse_categorical_accuracy: 0.9644 - val_loss: 0.1033 - val_sparse_categorical_accuracy: 0.9588\n",
      "Epoch 58/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0836 - sparse_categorical_accuracy: 0.9667Epoch 00057: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0832 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.1087 - val_sparse_categorical_accuracy: 0.9564\n",
      "Epoch 59/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0915 - sparse_categorical_accuracy: 0.9654Epoch 00058: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0912 - sparse_categorical_accuracy: 0.9656 - val_loss: 0.1151 - val_sparse_categorical_accuracy: 0.9547\n",
      "Epoch 60/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0962 - sparse_categorical_accuracy: 0.9625Epoch 00059: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0957 - sparse_categorical_accuracy: 0.9627 - val_loss: 0.1081 - val_sparse_categorical_accuracy: 0.9564\n",
      "Epoch 61/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0937 - sparse_categorical_accuracy: 0.9642Epoch 00060: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0932 - sparse_categorical_accuracy: 0.9644 - val_loss: 0.1059 - val_sparse_categorical_accuracy: 0.9580\n",
      "Epoch 62/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0838 - sparse_categorical_accuracy: 0.9652Epoch 00061: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0838 - sparse_categorical_accuracy: 0.9652 - val_loss: 0.1242 - val_sparse_categorical_accuracy: 0.9556\n",
      "Epoch 63/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0888 - sparse_categorical_accuracy: 0.9630Epoch 00062: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0883 - sparse_categorical_accuracy: 0.9632 - val_loss: 0.1079 - val_sparse_categorical_accuracy: 0.9539\n",
      "Epoch 64/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0891 - sparse_categorical_accuracy: 0.9632Epoch 00063: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0889 - sparse_categorical_accuracy: 0.9632 - val_loss: 0.1110 - val_sparse_categorical_accuracy: 0.9588\n",
      "Epoch 65/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0893 - sparse_categorical_accuracy: 0.9625Epoch 00064: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0899 - sparse_categorical_accuracy: 0.9623 - val_loss: 0.1078 - val_sparse_categorical_accuracy: 0.9572\n",
      "Epoch 66/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0850 - sparse_categorical_accuracy: 0.9667Epoch 00065: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0851 - sparse_categorical_accuracy: 0.9667 - val_loss: 0.1104 - val_sparse_categorical_accuracy: 0.9597\n",
      "Epoch 67/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0786 - sparse_categorical_accuracy: 0.9679Epoch 00066: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0784 - sparse_categorical_accuracy: 0.9679 - val_loss: 0.1053 - val_sparse_categorical_accuracy: 0.9580\n",
      "Epoch 68/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0742 - sparse_categorical_accuracy: 0.9714Epoch 00067: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0746 - sparse_categorical_accuracy: 0.9712 - val_loss: 0.1070 - val_sparse_categorical_accuracy: 0.9580\n",
      "Epoch 69/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0800 - sparse_categorical_accuracy: 0.9688Epoch 00068: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0808 - sparse_categorical_accuracy: 0.9685 - val_loss: 0.1109 - val_sparse_categorical_accuracy: 0.9564\n",
      "Epoch 70/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0771 - sparse_categorical_accuracy: 0.9685Epoch 00069: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0769 - sparse_categorical_accuracy: 0.9687 - val_loss: 0.1087 - val_sparse_categorical_accuracy: 0.9572\n",
      "Epoch 71/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0788 - sparse_categorical_accuracy: 0.9698Epoch 00070: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0785 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1082 - val_sparse_categorical_accuracy: 0.9588\n",
      "Epoch 72/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0827 - sparse_categorical_accuracy: 0.9677Epoch 00071: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0836 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.1045 - val_sparse_categorical_accuracy: 0.9605\n",
      "Epoch 73/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0813 - sparse_categorical_accuracy: 0.9665Epoch 00072: val_loss improved from 0.10329 to 0.10164, saving model to model72.h5\n",
      "4859/4859 [==============================] - 15s - loss: 0.0810 - sparse_categorical_accuracy: 0.9667 - val_loss: 0.1016 - val_sparse_categorical_accuracy: 0.9597\n",
      "Epoch 74/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0854 - sparse_categorical_accuracy: 0.9667Epoch 00073: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0852 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.1062 - val_sparse_categorical_accuracy: 0.9597\n",
      "Epoch 75/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0723 - sparse_categorical_accuracy: 0.9721Epoch 00074: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0721 - sparse_categorical_accuracy: 0.9722 - val_loss: 0.1045 - val_sparse_categorical_accuracy: 0.9597\n",
      "Epoch 76/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0759 - sparse_categorical_accuracy: 0.9688Epoch 00075: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0765 - sparse_categorical_accuracy: 0.9679 - val_loss: 0.1138 - val_sparse_categorical_accuracy: 0.9564\n",
      "Epoch 77/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0659 - sparse_categorical_accuracy: 0.9735Epoch 00076: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0656 - sparse_categorical_accuracy: 0.9737 - val_loss: 0.1143 - val_sparse_categorical_accuracy: 0.9564\n",
      "Epoch 78/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0890 - sparse_categorical_accuracy: 0.9681Epoch 00077: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0893 - sparse_categorical_accuracy: 0.9679 - val_loss: 0.1103 - val_sparse_categorical_accuracy: 0.9564\n",
      "Epoch 79/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0805 - sparse_categorical_accuracy: 0.9642Epoch 00078: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0801 - sparse_categorical_accuracy: 0.9644 - val_loss: 0.1071 - val_sparse_categorical_accuracy: 0.9572\n",
      "Epoch 80/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0775 - sparse_categorical_accuracy: 0.9688Epoch 00079: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0778 - sparse_categorical_accuracy: 0.9685 - val_loss: 0.1089 - val_sparse_categorical_accuracy: 0.9572\n",
      "Epoch 81/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0689 - sparse_categorical_accuracy: 0.9706Epoch 00080: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0687 - sparse_categorical_accuracy: 0.9708 - val_loss: 0.1061 - val_sparse_categorical_accuracy: 0.9605\n",
      "Epoch 82/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0754 - sparse_categorical_accuracy: 0.9702Epoch 00081: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0758 - sparse_categorical_accuracy: 0.9702 - val_loss: 0.1069 - val_sparse_categorical_accuracy: 0.9572\n",
      "Epoch 83/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0759 - sparse_categorical_accuracy: 0.9696Epoch 00082: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0759 - sparse_categorical_accuracy: 0.9695 - val_loss: 0.1050 - val_sparse_categorical_accuracy: 0.9597\n",
      "Epoch 84/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0660 - sparse_categorical_accuracy: 0.9758Epoch 00083: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0657 - sparse_categorical_accuracy: 0.9759 - val_loss: 0.1075 - val_sparse_categorical_accuracy: 0.9630\n",
      "Epoch 85/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0736 - sparse_categorical_accuracy: 0.9729Epoch 00084: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0734 - sparse_categorical_accuracy: 0.9730 - val_loss: 0.1085 - val_sparse_categorical_accuracy: 0.9580\n",
      "Epoch 86/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0674 - sparse_categorical_accuracy: 0.9723Epoch 00085: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0673 - sparse_categorical_accuracy: 0.9724 - val_loss: 0.1126 - val_sparse_categorical_accuracy: 0.9572\n",
      "Epoch 87/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0690 - sparse_categorical_accuracy: 0.9714Epoch 00086: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0689 - sparse_categorical_accuracy: 0.9716 - val_loss: 0.1144 - val_sparse_categorical_accuracy: 0.9597\n",
      "Epoch 88/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0758 - sparse_categorical_accuracy: 0.9683Epoch 00087: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0758 - sparse_categorical_accuracy: 0.9683 - val_loss: 0.1116 - val_sparse_categorical_accuracy: 0.9564\n",
      "Epoch 89/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0643 - sparse_categorical_accuracy: 0.9719Epoch 00088: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0641 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.1114 - val_sparse_categorical_accuracy: 0.9580\n",
      "Epoch 90/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0675 - sparse_categorical_accuracy: 0.9737Epoch 00089: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0684 - sparse_categorical_accuracy: 0.9732 - val_loss: 0.1137 - val_sparse_categorical_accuracy: 0.9580\n",
      "Epoch 91/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0660 - sparse_categorical_accuracy: 0.9745Epoch 00090: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0657 - sparse_categorical_accuracy: 0.9747 - val_loss: 0.1186 - val_sparse_categorical_accuracy: 0.9564\n",
      "Epoch 92/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0675 - sparse_categorical_accuracy: 0.9729Epoch 00091: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0672 - sparse_categorical_accuracy: 0.9730 - val_loss: 0.1129 - val_sparse_categorical_accuracy: 0.9597\n",
      "Epoch 93/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0550 - sparse_categorical_accuracy: 0.9791Epoch 00092: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0549 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.1215 - val_sparse_categorical_accuracy: 0.9556\n",
      "Epoch 94/1000\n",
      "4832/4859 [============================>.] - ETA: 0s - loss: 0.0668 - sparse_categorical_accuracy: 0.9735Epoch 00093: val_loss did not improve\n",
      "4859/4859 [==============================] - 15s - loss: 0.0666 - sparse_categorical_accuracy: 0.9737 - val_loss: 0.1170 - val_sparse_categorical_accuracy: 0.9547\n",
      "Epoch 00093: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f14085e2400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          batch_size=32, \n",
    "          epochs=1000, \n",
    "          verbose=1, \n",
    "          callbacks=callbacks_list, \n",
    "          validation_data=(X_val, y_val), \n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'sparse_categorical_accuracy']\n",
      "4832/4859 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.037017860682597346, 0.99073883527393425]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "trained_model = load_model(model_name)\n",
    "print(trained_model.metrics_names)\n",
    "trained_model.evaluate(x=X_train, y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184/1215 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10164280249123987, 0.95967078042128451]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.evaluate(x=X_val, y=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
