{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(135109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sensor = '59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    X = []\n",
    "    y = []\n",
    "    data_dir_0 = sorted(os.listdir(os.path.join('pac_data', sensor, '0')))   \n",
    "    i = 0\n",
    "    for f in data_dir_0:\n",
    "        if i % 100 == 0: print(i)\n",
    "        i += 1\n",
    "        X.append(np.load(os.path.join('pac_data', sensor, '0', f))['x'])\n",
    "        y.append(0)\n",
    "    X = np.asarray(X, dtype=np.float16)\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X[indices[:int(0.8*len(indices))]]\n",
    "    y_train = y[indices[:int(0.8*len(indices))]]\n",
    "    X_val = X[indices[int(0.8*len(indices)):]]\n",
    "    y_val = y[indices[int(0.8*len(indices)):]]\n",
    "    labels_present = os.listdir(os.path.join('pac_data', sensor))\n",
    "    if '1' in labels_present:\n",
    "        X = []\n",
    "        y = []\n",
    "        data_dir_1 = sorted(os.listdir(os.path.join('pac_data', sensor, '1')))\n",
    "        i = 0\n",
    "        for f in data_dir_1:\n",
    "            if i % 100 == 0: print(i)\n",
    "            i += 1\n",
    "            X.append(np.load(os.path.join('pac_data', sensor, '1', f))['x'])\n",
    "            y.append(1)        \n",
    "        X = np.asarray(X, dtype=np.float16)\n",
    "        y = np.asarray(y, dtype=int)\n",
    "        indices = np.arange(y.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = np.concatenate([X_train, X[indices[:int(0.8*len(indices))]]])\n",
    "        y_train = np.concatenate([y_train, y[indices[:int(0.8*len(indices))]]])\n",
    "        X_val = np.concatenate([X_val, X[indices[int(0.8*len(indices)):]]])\n",
    "        y_val = np.concatenate([y_val, y[indices[int(0.8*len(indices)):]]])\n",
    "    X_train = np.expand_dims(np.asarray(X_train, dtype=np.float16), axis=3)\n",
    "    y_train = np.asarray(y_train, dtype=int)\n",
    "    X_val = np.expand_dims(np.asarray(X_val, dtype=np.float16), axis=3)\n",
    "    y_val = np.asarray(y_val, dtype=int)\n",
    "    X_train /= 5.\n",
    "    X_val /= 5.\n",
    "    mean_arr = np.mean(X_train, axis = 0)\n",
    "    np.save(os.path.join('pac_data', sensor, 'mean.npy'), mean_arr)\n",
    "    X_train -= mean_arr \n",
    "    X_val -= mean_arr  \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "0\n",
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12929, 240, 320, 1) (12929,) (3233, 240, 320, 1) (3233,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling2D, Activation, Dense, Input\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers import concatenate\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import conv_utils\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras.applications.xception import Xception\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#linear\n",
    "def linear(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, x, name='linear')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef xception(dropout):\\n    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\\n    x = base_model.output\\n    x = Flatten()(x)\\n    x = Dropout(dropout)(x) # drop prob\\n    predictions = Dense(2, activation='softmax')(x)\\n    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\\n    return model\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import SeparableConv2D\n",
    "\n",
    "def xception(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))    \n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2), use_bias=False, name='block1_conv1')(inputs)\n",
    "    x = BatchNormalization(name='block1_conv1_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv1_act')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', use_bias=False, name='block1_conv2')(x)\n",
    "    x = BatchNormalization(name='block1_conv2_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv2_act')(x)\n",
    "\n",
    "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block2_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(256, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block3_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block3_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(728, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block4_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block4_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    for i in range(8):\n",
    "        residual = x\n",
    "        prefix = 'block' + str(i + 5)\n",
    "\n",
    "        x = Activation('relu', name=prefix + '_sepconv1_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv1_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv2_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv2_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv3_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv3_bn')(x)\n",
    "\n",
    "        x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(1024, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block13_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block13_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block13_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv1_act')(x)\n",
    "\n",
    "    x = SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv2_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv2_act')(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "def xception(dropout):\n",
    "    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x) # drop prob\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vanillacnn(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1)) \n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 240, 320, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 120, 160, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 120, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 120, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 40, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 10, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 10, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 10, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 390,786\n",
      "Trainable params: 389,826\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model' + sensor + '.h5'\n",
    "model = vanillacnn(0.5)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "stop = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, stop]\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12929 samples, validate on 3233 samples\n",
      "Epoch 1/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0989 - sparse_categorical_accuracy: 0.9814Epoch 00000: val_loss improved from inf to 0.14917, saving model to model59.h5\n",
      "12929/12929 [==============================] - 42s - loss: 0.0989 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.1492 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 2/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0679 - sparse_categorical_accuracy: 0.9844Epoch 00001: val_loss improved from 0.14917 to 0.05199, saving model to model59.h5\n",
      "12929/12929 [==============================] - 41s - loss: 0.0679 - sparse_categorical_accuracy: 0.9844 - val_loss: 0.0520 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 3/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0553 - sparse_categorical_accuracy: 0.9861Epoch 00002: val_loss improved from 0.05199 to 0.04087, saving model to model59.h5\n",
      "12929/12929 [==============================] - 41s - loss: 0.0553 - sparse_categorical_accuracy: 0.9861 - val_loss: 0.0409 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 4/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0453 - sparse_categorical_accuracy: 0.9872Epoch 00003: val_loss improved from 0.04087 to 0.03166, saving model to model59.h5\n",
      "12929/12929 [==============================] - 41s - loss: 0.0453 - sparse_categorical_accuracy: 0.9872 - val_loss: 0.0317 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 5/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0419 - sparse_categorical_accuracy: 0.9879Epoch 00004: val_loss improved from 0.03166 to 0.02787, saving model to model59.h5\n",
      "12929/12929 [==============================] - 41s - loss: 0.0419 - sparse_categorical_accuracy: 0.9879 - val_loss: 0.0279 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 6/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0373 - sparse_categorical_accuracy: 0.9879Epoch 00005: val_loss improved from 0.02787 to 0.02026, saving model to model59.h5\n",
      "12929/12929 [==============================] - 41s - loss: 0.0373 - sparse_categorical_accuracy: 0.9879 - val_loss: 0.0203 - val_sparse_categorical_accuracy: 0.9920\n",
      "Epoch 7/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0322 - sparse_categorical_accuracy: 0.9901Epoch 00006: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0322 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 8/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0300 - sparse_categorical_accuracy: 0.9899Epoch 00007: val_loss improved from 0.02026 to 0.01580, saving model to model59.h5\n",
      "12929/12929 [==============================] - 41s - loss: 0.0300 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0158 - val_sparse_categorical_accuracy: 0.9923\n",
      "Epoch 9/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0265 - sparse_categorical_accuracy: 0.9903Epoch 00008: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0265 - sparse_categorical_accuracy: 0.9903 - val_loss: 0.0618 - val_sparse_categorical_accuracy: 0.9898\n",
      "Epoch 10/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0235 - sparse_categorical_accuracy: 0.9915Epoch 00009: val_loss improved from 0.01580 to 0.01384, saving model to model59.h5\n",
      "12929/12929 [==============================] - 41s - loss: 0.0235 - sparse_categorical_accuracy: 0.9915 - val_loss: 0.0138 - val_sparse_categorical_accuracy: 0.9944\n",
      "Epoch 11/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0218 - sparse_categorical_accuracy: 0.9913Epoch 00010: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0218 - sparse_categorical_accuracy: 0.9913 - val_loss: 0.0203 - val_sparse_categorical_accuracy: 0.9920\n",
      "Epoch 12/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0237 - sparse_categorical_accuracy: 0.9924Epoch 00011: val_loss improved from 0.01384 to 0.01176, saving model to model59.h5\n",
      "12929/12929 [==============================] - 41s - loss: 0.0237 - sparse_categorical_accuracy: 0.9924 - val_loss: 0.0118 - val_sparse_categorical_accuracy: 0.9954\n",
      "Epoch 13/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0209 - sparse_categorical_accuracy: 0.9916Epoch 00012: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0209 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0143 - val_sparse_categorical_accuracy: 0.9926\n",
      "Epoch 14/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0191 - sparse_categorical_accuracy: 0.9933Epoch 00013: val_loss improved from 0.01176 to 0.00731, saving model to model59.h5\n",
      "12929/12929 [==============================] - 41s - loss: 0.0191 - sparse_categorical_accuracy: 0.9933 - val_loss: 0.0073 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 15/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0190 - sparse_categorical_accuracy: 0.9923Epoch 00014: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0190 - sparse_categorical_accuracy: 0.9923 - val_loss: 0.0215 - val_sparse_categorical_accuracy: 0.9870\n",
      "Epoch 16/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0162 - sparse_categorical_accuracy: 0.9944Epoch 00015: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0162 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.0142 - val_sparse_categorical_accuracy: 0.9920\n",
      "Epoch 17/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0177 - sparse_categorical_accuracy: 0.9937Epoch 00016: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0177 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.0110 - val_sparse_categorical_accuracy: 0.9957\n",
      "Epoch 18/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0173 - sparse_categorical_accuracy: 0.9937Epoch 00017: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0173 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.0130 - val_sparse_categorical_accuracy: 0.9947\n",
      "Epoch 19/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0152 - sparse_categorical_accuracy: 0.9947Epoch 00018: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0152 - sparse_categorical_accuracy: 0.9947 - val_loss: 0.0147 - val_sparse_categorical_accuracy: 0.9907\n",
      "Epoch 20/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0152 - sparse_categorical_accuracy: 0.9937Epoch 00019: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0152 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.0209 - val_sparse_categorical_accuracy: 0.9879\n",
      "Epoch 21/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0150 - sparse_categorical_accuracy: 0.9945Epoch 00020: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0150 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0103 - val_sparse_categorical_accuracy: 0.9938\n",
      "Epoch 22/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0159 - sparse_categorical_accuracy: 0.9938Epoch 00021: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0159 - sparse_categorical_accuracy: 0.9938 - val_loss: 0.0096 - val_sparse_categorical_accuracy: 0.9963\n",
      "Epoch 23/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0136 - sparse_categorical_accuracy: 0.9946Epoch 00022: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0136 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.0160 - val_sparse_categorical_accuracy: 0.9889\n",
      "Epoch 24/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0124 - sparse_categorical_accuracy: 0.9950Epoch 00023: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0124 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.0120 - val_sparse_categorical_accuracy: 0.9944\n",
      "Epoch 25/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0107 - sparse_categorical_accuracy: 0.9964Epoch 00024: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0107 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0099 - val_sparse_categorical_accuracy: 0.9957\n",
      "Epoch 26/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0112 - sparse_categorical_accuracy: 0.9957Epoch 00025: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0112 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.0428 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 27/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0148 - sparse_categorical_accuracy: 0.9942Epoch 00026: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0148 - sparse_categorical_accuracy: 0.9942 - val_loss: 0.0118 - val_sparse_categorical_accuracy: 0.9932\n",
      "Epoch 28/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0092 - sparse_categorical_accuracy: 0.9962Epoch 00027: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0092 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.0087 - val_sparse_categorical_accuracy: 0.9963\n",
      "Epoch 29/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0132 - sparse_categorical_accuracy: 0.9951Epoch 00028: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0132 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.0078 - val_sparse_categorical_accuracy: 0.9966\n",
      "Epoch 30/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0110 - sparse_categorical_accuracy: 0.9962Epoch 00029: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0114 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.0117 - val_sparse_categorical_accuracy: 0.9951\n",
      "Epoch 31/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0165 - sparse_categorical_accuracy: 0.9940Epoch 00030: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0165 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0100 - val_sparse_categorical_accuracy: 0.9957\n",
      "Epoch 32/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0099 - sparse_categorical_accuracy: 0.9958Epoch 00031: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0099 - sparse_categorical_accuracy: 0.9958 - val_loss: 0.0119 - val_sparse_categorical_accuracy: 0.9938\n",
      "Epoch 33/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0117 - sparse_categorical_accuracy: 0.9955Epoch 00032: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0117 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.0076 - val_sparse_categorical_accuracy: 0.9972\n",
      "Epoch 34/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0106 - sparse_categorical_accuracy: 0.9954Epoch 00033: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0106 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.0127 - val_sparse_categorical_accuracy: 0.9932\n",
      "Epoch 35/1000\n",
      "12928/12929 [============================>.] - ETA: 0s - loss: 0.0103 - sparse_categorical_accuracy: 0.9961Epoch 00034: val_loss did not improve\n",
      "12929/12929 [==============================] - 41s - loss: 0.0103 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.0126 - val_sparse_categorical_accuracy: 0.9951\n",
      "Epoch 00034: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff02b29e9b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          batch_size=32, \n",
    "          epochs=1000, \n",
    "          verbose=1, \n",
    "          callbacks=callbacks_list, \n",
    "          validation_data=(X_val, y_val), \n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'sparse_categorical_accuracy']\n",
      "12896/12929 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0085889953809831793, 0.99706087091035656]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "trained_model = load_model(model_name)\n",
    "print(trained_model.metrics_names)\n",
    "trained_model.evaluate(x=X_train, y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3233 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0073128274994300755, 0.99752551809464896]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.evaluate(x=X_val, y=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
