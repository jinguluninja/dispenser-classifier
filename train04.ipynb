{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(135109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sensor = '04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    X = []\n",
    "    y = []\n",
    "    data_dir_0 = sorted(os.listdir(os.path.join('pac_data', sensor, '0')))   \n",
    "    i = 0\n",
    "    for f in data_dir_0:\n",
    "        if i % 100 == 0: print(i)\n",
    "        i += 1\n",
    "        X.append(np.load(os.path.join('pac_data', sensor, '0', f))['x'])\n",
    "        y.append(0)\n",
    "    X = np.asarray(X, dtype=np.float16)\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X[indices[:int(0.8*len(indices))]]\n",
    "    y_train = y[indices[:int(0.8*len(indices))]]\n",
    "    X_val = X[indices[int(0.8*len(indices)):]]\n",
    "    y_val = y[indices[int(0.8*len(indices)):]]\n",
    "    labels_present = os.listdir(os.path.join('pac_data', sensor))\n",
    "    if '1' in labels_present:\n",
    "        X = []\n",
    "        y = []\n",
    "        data_dir_1 = sorted(os.listdir(os.path.join('pac_data', sensor, '1')))\n",
    "        i = 0\n",
    "        for f in data_dir_1:\n",
    "            if i % 100 == 0: print(i)\n",
    "            i += 1\n",
    "            X.append(np.load(os.path.join('pac_data', sensor, '1', f))['x'])\n",
    "            y.append(1)        \n",
    "        X = np.asarray(X, dtype=np.float16)\n",
    "        y = np.asarray(y, dtype=int)\n",
    "        indices = np.arange(y.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = np.concatenate([X_train, X[indices[:int(0.8*len(indices))]]])\n",
    "        y_train = np.concatenate([y_train, y[indices[:int(0.8*len(indices))]]])\n",
    "        X_val = np.concatenate([X_val, X[indices[int(0.8*len(indices)):]]])\n",
    "        y_val = np.concatenate([y_val, y[indices[int(0.8*len(indices)):]]])\n",
    "    X_train = np.expand_dims(np.asarray(X_train, dtype=np.float16), axis=3)\n",
    "    y_train = np.asarray(y_train, dtype=int)\n",
    "    X_val = np.expand_dims(np.asarray(X_val, dtype=np.float16), axis=3)\n",
    "    y_val = np.asarray(y_val, dtype=int)\n",
    "    X_train /= 5.\n",
    "    X_val /= 5.\n",
    "    mean_arr = np.mean(X_train, axis = 0)\n",
    "    np.save(os.path.join('pac_data', sensor, 'mean.npy'), mean_arr)\n",
    "    X_train -= mean_arr \n",
    "    X_val -= mean_arr  \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14940, 240, 320, 1) (14940,) (3737, 240, 320, 1) (3737,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling2D, Activation, Dense, Input\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers import concatenate\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import conv_utils\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras.applications.xception import Xception\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#linear\n",
    "def linear(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, x, name='linear')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef xception(dropout):\\n    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\\n    x = base_model.output\\n    x = Flatten()(x)\\n    x = Dropout(dropout)(x) # drop prob\\n    predictions = Dense(2, activation='softmax')(x)\\n    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\\n    return model\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import SeparableConv2D\n",
    "\n",
    "def xception(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))    \n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2), use_bias=False, name='block1_conv1')(inputs)\n",
    "    x = BatchNormalization(name='block1_conv1_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv1_act')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', use_bias=False, name='block1_conv2')(x)\n",
    "    x = BatchNormalization(name='block1_conv2_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv2_act')(x)\n",
    "\n",
    "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block2_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(256, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block3_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block3_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(728, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block4_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block4_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    for i in range(8):\n",
    "        residual = x\n",
    "        prefix = 'block' + str(i + 5)\n",
    "\n",
    "        x = Activation('relu', name=prefix + '_sepconv1_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv1_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv2_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv2_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv3_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv3_bn')(x)\n",
    "\n",
    "        x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(1024, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block13_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block13_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block13_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv1_act')(x)\n",
    "\n",
    "    x = SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv2_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv2_act')(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "def xception(dropout):\n",
    "    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x) # drop prob\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vanillacnn(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1)) \n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 240, 320, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 120, 160, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 120, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 120, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 40, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 10, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 10, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 10, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 390,786\n",
      "Trainable params: 389,826\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model' + sensor + '.h5'\n",
    "model = vanillacnn(0.5)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "stop = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, stop]\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14940 samples, validate on 3737 samples\n",
      "Epoch 1/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.1807 - sparse_categorical_accuracy: 0.9392Epoch 00000: val_loss improved from inf to 0.73190, saving model to model04.h5\n",
      "14940/14940 [==============================] - 49s - loss: 0.1806 - sparse_categorical_accuracy: 0.9392 - val_loss: 0.7319 - val_sparse_categorical_accuracy: 0.0945\n",
      "Epoch 2/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.1004 - sparse_categorical_accuracy: 0.9603Epoch 00001: val_loss improved from 0.73190 to 0.15215, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.1007 - sparse_categorical_accuracy: 0.9602 - val_loss: 0.1521 - val_sparse_categorical_accuracy: 0.9732\n",
      "Epoch 3/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0762 - sparse_categorical_accuracy: 0.9698Epoch 00002: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0762 - sparse_categorical_accuracy: 0.9697 - val_loss: 0.3113 - val_sparse_categorical_accuracy: 0.9762\n",
      "Epoch 4/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0651 - sparse_categorical_accuracy: 0.9749Epoch 00003: val_loss improved from 0.15215 to 0.05912, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0651 - sparse_categorical_accuracy: 0.9749 - val_loss: 0.0591 - val_sparse_categorical_accuracy: 0.9789\n",
      "Epoch 5/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0535 - sparse_categorical_accuracy: 0.9800Epoch 00004: val_loss improved from 0.05912 to 0.04113, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0536 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.0411 - val_sparse_categorical_accuracy: 0.9874\n",
      "Epoch 6/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0522 - sparse_categorical_accuracy: 0.9810Epoch 00005: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0522 - sparse_categorical_accuracy: 0.9810 - val_loss: 0.0428 - val_sparse_categorical_accuracy: 0.9861\n",
      "Epoch 7/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0468 - sparse_categorical_accuracy: 0.9839Epoch 00006: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0469 - sparse_categorical_accuracy: 0.9839 - val_loss: 0.0427 - val_sparse_categorical_accuracy: 0.9888\n",
      "Epoch 8/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0423 - sparse_categorical_accuracy: 0.9846Epoch 00007: val_loss improved from 0.04113 to 0.03009, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0424 - sparse_categorical_accuracy: 0.9846 - val_loss: 0.0301 - val_sparse_categorical_accuracy: 0.9914\n",
      "Epoch 9/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0396 - sparse_categorical_accuracy: 0.9852Epoch 00008: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0396 - sparse_categorical_accuracy: 0.9853 - val_loss: 0.0651 - val_sparse_categorical_accuracy: 0.9885\n",
      "Epoch 10/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0388 - sparse_categorical_accuracy: 0.9867Epoch 00009: val_loss improved from 0.03009 to 0.02930, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0388 - sparse_categorical_accuracy: 0.9867 - val_loss: 0.0293 - val_sparse_categorical_accuracy: 0.9893\n",
      "Epoch 11/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0374 - sparse_categorical_accuracy: 0.9869Epoch 00010: val_loss improved from 0.02930 to 0.02884, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0373 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.0288 - val_sparse_categorical_accuracy: 0.9893\n",
      "Epoch 12/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0366 - sparse_categorical_accuracy: 0.9873Epoch 00011: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0366 - sparse_categorical_accuracy: 0.9873 - val_loss: 0.0298 - val_sparse_categorical_accuracy: 0.9885\n",
      "Epoch 13/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0346 - sparse_categorical_accuracy: 0.9885Epoch 00012: val_loss improved from 0.02884 to 0.02552, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0345 - sparse_categorical_accuracy: 0.9885 - val_loss: 0.0255 - val_sparse_categorical_accuracy: 0.9906\n",
      "Epoch 14/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0348 - sparse_categorical_accuracy: 0.9887Epoch 00013: val_loss improved from 0.02552 to 0.02428, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0347 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0243 - val_sparse_categorical_accuracy: 0.9909\n",
      "Epoch 15/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0349 - sparse_categorical_accuracy: 0.9883Epoch 00014: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0348 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0259 - val_sparse_categorical_accuracy: 0.9914\n",
      "Epoch 16/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0324 - sparse_categorical_accuracy: 0.9887Epoch 00015: val_loss improved from 0.02428 to 0.02365, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0323 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0236 - val_sparse_categorical_accuracy: 0.9914\n",
      "Epoch 17/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0306 - sparse_categorical_accuracy: 0.9894Epoch 00016: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0306 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.0242 - val_sparse_categorical_accuracy: 0.9906\n",
      "Epoch 18/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0287 - sparse_categorical_accuracy: 0.9907Epoch 00017: val_loss improved from 0.02365 to 0.02352, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0287 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.0235 - val_sparse_categorical_accuracy: 0.9920\n",
      "Epoch 19/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0277 - sparse_categorical_accuracy: 0.9905Epoch 00018: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0277 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.0246 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 20/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0289 - sparse_categorical_accuracy: 0.9903Epoch 00019: val_loss improved from 0.02352 to 0.02267, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0289 - sparse_categorical_accuracy: 0.9904 - val_loss: 0.0227 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 21/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0267 - sparse_categorical_accuracy: 0.9909Epoch 00020: val_loss improved from 0.02267 to 0.02103, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0267 - sparse_categorical_accuracy: 0.9909 - val_loss: 0.0210 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 22/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0289 - sparse_categorical_accuracy: 0.9905Epoch 00021: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0289 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.0225 - val_sparse_categorical_accuracy: 0.9920\n",
      "Epoch 23/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0264 - sparse_categorical_accuracy: 0.9906Epoch 00022: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0263 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.0241 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 24/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0254 - sparse_categorical_accuracy: 0.9914Epoch 00023: val_loss improved from 0.02103 to 0.02070, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0254 - sparse_categorical_accuracy: 0.9914 - val_loss: 0.0207 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 25/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0258 - sparse_categorical_accuracy: 0.9911Epoch 00024: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0257 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0210 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 26/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0243 - sparse_categorical_accuracy: 0.9920Epoch 00025: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0242 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.0224 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 27/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0254 - sparse_categorical_accuracy: 0.9912Epoch 00026: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0259 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0213 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 28/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0213 - sparse_categorical_accuracy: 0.9928Epoch 00027: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0212 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0225 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 29/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0239 - sparse_categorical_accuracy: 0.9928Epoch 00028: val_loss improved from 0.02070 to 0.02042, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0238 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0204 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 30/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0236 - sparse_categorical_accuracy: 0.9916Epoch 00029: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0236 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0205 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 31/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0216 - sparse_categorical_accuracy: 0.9927Epoch 00030: val_loss improved from 0.02042 to 0.02037, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0216 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.0204 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 32/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0220 - sparse_categorical_accuracy: 0.9925Epoch 00031: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0219 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.0207 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 33/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0228 - sparse_categorical_accuracy: 0.9922Epoch 00032: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0230 - sparse_categorical_accuracy: 0.9922 - val_loss: 0.0224 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 34/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0217 - sparse_categorical_accuracy: 0.9924Epoch 00033: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0217 - sparse_categorical_accuracy: 0.9924 - val_loss: 0.0219 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 35/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0228 - sparse_categorical_accuracy: 0.9931Epoch 00034: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0228 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 36/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0217 - sparse_categorical_accuracy: 0.9925Epoch 00035: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0217 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.0217 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 37/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0209 - sparse_categorical_accuracy: 0.9928Epoch 00036: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0212 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0208 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 38/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0226 - sparse_categorical_accuracy: 0.9925Epoch 00037: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0226 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.0208 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 39/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0204 - sparse_categorical_accuracy: 0.9934Epoch 00038: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0203 - sparse_categorical_accuracy: 0.9934 - val_loss: 0.0223 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 40/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0189 - sparse_categorical_accuracy: 0.9936Epoch 00039: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0194 - sparse_categorical_accuracy: 0.9934 - val_loss: 0.0214 - val_sparse_categorical_accuracy: 0.9922\n",
      "Epoch 41/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0201 - sparse_categorical_accuracy: 0.9934Epoch 00040: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0201 - sparse_categorical_accuracy: 0.9934 - val_loss: 0.0234 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 42/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0204 - sparse_categorical_accuracy: 0.9929Epoch 00041: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0203 - sparse_categorical_accuracy: 0.9929 - val_loss: 0.0206 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 43/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0195 - sparse_categorical_accuracy: 0.9940Epoch 00042: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0198 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0216 - val_sparse_categorical_accuracy: 0.9920\n",
      "Epoch 44/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0208 - sparse_categorical_accuracy: 0.9931Epoch 00043: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0207 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 45/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0183 - sparse_categorical_accuracy: 0.9936Epoch 00044: val_loss improved from 0.02037 to 0.01959, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0182 - sparse_categorical_accuracy: 0.9936 - val_loss: 0.0196 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 46/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0215 - sparse_categorical_accuracy: 0.9937Epoch 00045: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0215 - sparse_categorical_accuracy: 0.9936 - val_loss: 0.0196 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 47/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0197 - sparse_categorical_accuracy: 0.9932Epoch 00046: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0197 - sparse_categorical_accuracy: 0.9932 - val_loss: 0.0217 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 48/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0186 - sparse_categorical_accuracy: 0.9938Epoch 00047: val_loss did not improve\n",
      "14940/14940 [==============================] - 47s - loss: 0.0186 - sparse_categorical_accuracy: 0.9938 - val_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9922\n",
      "Epoch 49/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0178 - sparse_categorical_accuracy: 0.9936Epoch 00048: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0178 - sparse_categorical_accuracy: 0.9936 - val_loss: 0.0205 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 50/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0190 - sparse_categorical_accuracy: 0.9940Epoch 00049: val_loss did not improve\n",
      "14940/14940 [==============================] - 47s - loss: 0.0190 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0222 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 51/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0213 - sparse_categorical_accuracy: 0.9932Epoch 00050: val_loss did not improve\n",
      "14940/14940 [==============================] - 47s - loss: 0.0213 - sparse_categorical_accuracy: 0.9932 - val_loss: 0.0209 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 52/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0169 - sparse_categorical_accuracy: 0.9939Epoch 00051: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0169 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.0201 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 53/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0189 - sparse_categorical_accuracy: 0.9940Epoch 00052: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0189 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0204 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 54/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0182 - sparse_categorical_accuracy: 0.9931Epoch 00053: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0182 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0218 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 55/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0163 - sparse_categorical_accuracy: 0.9946Epoch 00054: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0163 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.0232 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 56/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0189 - sparse_categorical_accuracy: 0.9939Epoch 00055: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0189 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.0204 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 57/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0169 - sparse_categorical_accuracy: 0.9940Epoch 00056: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0168 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0197 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 58/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0184 - sparse_categorical_accuracy: 0.9938Epoch 00057: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0184 - sparse_categorical_accuracy: 0.9938 - val_loss: 0.0205 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 59/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0161 - sparse_categorical_accuracy: 0.9940Epoch 00058: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0163 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0198 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 60/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0173 - sparse_categorical_accuracy: 0.9937Epoch 00059: val_loss improved from 0.01959 to 0.01932, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0173 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.0193 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 61/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0147 - sparse_categorical_accuracy: 0.9949Epoch 00060: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0146 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.0195 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 62/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0170 - sparse_categorical_accuracy: 0.9938Epoch 00061: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0170 - sparse_categorical_accuracy: 0.9938 - val_loss: 0.0208 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 63/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0177 - sparse_categorical_accuracy: 0.9941Epoch 00062: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0177 - sparse_categorical_accuracy: 0.9941 - val_loss: 0.0195 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 64/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0155 - sparse_categorical_accuracy: 0.9943Epoch 00063: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0155 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.0207 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 65/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0163 - sparse_categorical_accuracy: 0.9940Epoch 00064: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0163 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0206 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 66/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0145 - sparse_categorical_accuracy: 0.9945Epoch 00065: val_loss improved from 0.01932 to 0.01921, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0145 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0192 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 67/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0174 - sparse_categorical_accuracy: 0.9937Epoch 00066: val_loss improved from 0.01921 to 0.01893, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0175 - sparse_categorical_accuracy: 0.9936 - val_loss: 0.0189 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 68/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0162 - sparse_categorical_accuracy: 0.9943Epoch 00067: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0163 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.0196 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 69/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0160 - sparse_categorical_accuracy: 0.9946Epoch 00068: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0161 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.0191 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 70/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0158 - sparse_categorical_accuracy: 0.9945Epoch 00069: val_loss improved from 0.01893 to 0.01863, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0158 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0186 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 71/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0161 - sparse_categorical_accuracy: 0.9950Epoch 00070: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0162 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.0190 - val_sparse_categorical_accuracy: 0.9922\n",
      "Epoch 72/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0160 - sparse_categorical_accuracy: 0.9943Epoch 00071: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0160 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.0189 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 73/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0141 - sparse_categorical_accuracy: 0.9944Epoch 00072: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0140 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.0191 - val_sparse_categorical_accuracy: 0.9936\n",
      "Epoch 74/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0141 - sparse_categorical_accuracy: 0.9953Epoch 00073: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0141 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.0188 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 75/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0155 - sparse_categorical_accuracy: 0.9940Epoch 00074: val_loss improved from 0.01863 to 0.01846, saving model to model04.h5\n",
      "14940/14940 [==============================] - 48s - loss: 0.0155 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0185 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 76/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0134 - sparse_categorical_accuracy: 0.9954Epoch 00075: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0135 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.0187 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 77/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0151 - sparse_categorical_accuracy: 0.9943Epoch 00076: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0151 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.0204 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 78/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0157 - sparse_categorical_accuracy: 0.9937Epoch 00077: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0157 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.0201 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 79/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0144 - sparse_categorical_accuracy: 0.9948Epoch 00078: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0144 - sparse_categorical_accuracy: 0.9948 - val_loss: 0.0208 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 80/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0143 - sparse_categorical_accuracy: 0.9944Epoch 00079: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0143 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.0189 - val_sparse_categorical_accuracy: 0.9941\n",
      "Epoch 81/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0137 - sparse_categorical_accuracy: 0.9950Epoch 00080: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0137 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.0206 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 82/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0148 - sparse_categorical_accuracy: 0.9945Epoch 00081: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0148 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0189 - val_sparse_categorical_accuracy: 0.9938\n",
      "Epoch 83/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0134 - sparse_categorical_accuracy: 0.9950Epoch 00082: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0134 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.0192 - val_sparse_categorical_accuracy: 0.9936\n",
      "Epoch 84/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0138 - sparse_categorical_accuracy: 0.9948Epoch 00083: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0138 - sparse_categorical_accuracy: 0.9948 - val_loss: 0.0203 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 85/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0142 - sparse_categorical_accuracy: 0.9949Epoch 00084: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0142 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.0205 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 86/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0133 - sparse_categorical_accuracy: 0.9954Epoch 00085: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0132 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.0196 - val_sparse_categorical_accuracy: 0.9936\n",
      "Epoch 87/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0122 - sparse_categorical_accuracy: 0.9952Epoch 00086: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0122 - sparse_categorical_accuracy: 0.9952 - val_loss: 0.0203 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 88/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0135 - sparse_categorical_accuracy: 0.9953Epoch 00087: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0135 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.0215 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 89/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0143 - sparse_categorical_accuracy: 0.9949Epoch 00088: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0143 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.0201 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 90/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0113 - sparse_categorical_accuracy: 0.9953Epoch 00089: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0113 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.0194 - val_sparse_categorical_accuracy: 0.9938\n",
      "Epoch 91/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0134 - sparse_categorical_accuracy: 0.9953Epoch 00090: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0134 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.0200 - val_sparse_categorical_accuracy: 0.9936\n",
      "Epoch 92/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0137 - sparse_categorical_accuracy: 0.9952Epoch 00091: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0137 - sparse_categorical_accuracy: 0.9952 - val_loss: 0.0202 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 93/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0147 - sparse_categorical_accuracy: 0.9946Epoch 00092: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0147 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.0200 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 94/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0124 - sparse_categorical_accuracy: 0.9962Epoch 00093: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0124 - sparse_categorical_accuracy: 0.9963 - val_loss: 0.0209 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 95/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0153 - sparse_categorical_accuracy: 0.9949Epoch 00094: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0153 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.0206 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 96/1000\n",
      "14912/14940 [============================>.] - ETA: 0s - loss: 0.0120 - sparse_categorical_accuracy: 0.9964Epoch 00095: val_loss did not improve\n",
      "14940/14940 [==============================] - 48s - loss: 0.0120 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.0216 - val_sparse_categorical_accuracy: 0.9925\n",
      "Epoch 00095: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f273692f2e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          batch_size=32, \n",
    "          epochs=1000, \n",
    "          verbose=1, \n",
    "          callbacks=callbacks_list, \n",
    "          validation_data=(X_val, y_val), \n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'sparse_categorical_accuracy']\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "trained_model = load_model(model_name)\n",
    "print(trained_model.metrics_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14912/14940 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0077658456025452745, 0.99799196787148592]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.evaluate(x=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3737/3737 [==============================] - 5s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.018457461629118623, 0.99277495302744401]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.evaluate(x=X_val, y=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
