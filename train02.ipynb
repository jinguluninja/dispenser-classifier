{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(135109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sensor = '02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    X = []\n",
    "    y = []\n",
    "    data_dir_0 = sorted(os.listdir(os.path.join('pac_data', sensor, '0')))   \n",
    "    i = 0\n",
    "    for f in data_dir_0:\n",
    "        if i % 100 == 0: print(i)\n",
    "        i += 1\n",
    "        X.append(np.load(os.path.join('pac_data', sensor, '0', f))['x'])\n",
    "        y.append(0)\n",
    "    X = np.asarray(X, dtype=np.float16)\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X[indices[:int(0.8*len(indices))]]\n",
    "    y_train = y[indices[:int(0.8*len(indices))]]\n",
    "    X_val = X[indices[int(0.8*len(indices)):]]\n",
    "    y_val = y[indices[int(0.8*len(indices)):]]\n",
    "    labels_present = os.listdir(os.path.join('pac_data', sensor))\n",
    "    if '1' in labels_present:\n",
    "        X = []\n",
    "        y = []\n",
    "        data_dir_1 = sorted(os.listdir(os.path.join('pac_data', sensor, '1')))\n",
    "        i = 0\n",
    "        for f in data_dir_1:\n",
    "            if i % 100 == 0: print(i)\n",
    "            i += 1\n",
    "            X.append(np.load(os.path.join('pac_data', sensor, '1', f))['x'])\n",
    "            y.append(1)        \n",
    "        X = np.asarray(X, dtype=np.float16)\n",
    "        y = np.asarray(y, dtype=int)\n",
    "        indices = np.arange(y.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = np.concatenate([X_train, X[indices[:int(0.8*len(indices))]]])\n",
    "        y_train = np.concatenate([y_train, y[indices[:int(0.8*len(indices))]]])\n",
    "        X_val = np.concatenate([X_val, X[indices[int(0.8*len(indices)):]]])\n",
    "        y_val = np.concatenate([y_val, y[indices[int(0.8*len(indices)):]]])\n",
    "    X_train = np.expand_dims(np.asarray(X_train, dtype=np.float16), axis=3)\n",
    "    y_train = np.asarray(y_train, dtype=int)\n",
    "    X_val = np.expand_dims(np.asarray(X_val, dtype=np.float16), axis=3)\n",
    "    y_val = np.asarray(y_val, dtype=int)\n",
    "    X_train /= 5.\n",
    "    X_val /= 5.\n",
    "    mean_arr = np.mean(X_train, axis = 0)\n",
    "    np.save(os.path.join('pac_data', sensor, 'mean.npy'), mean_arr)\n",
    "    X_train -= mean_arr \n",
    "    X_val -= mean_arr  \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9080, 240, 320, 1) (9080,) (2271, 240, 320, 1) (2271,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling2D, Activation, Dense, Input\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers import concatenate\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import conv_utils\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras.applications.xception import Xception\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#linear\n",
    "def linear(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, x, name='linear')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef xception(dropout):\\n    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\\n    x = base_model.output\\n    x = Flatten()(x)\\n    x = Dropout(dropout)(x) # drop prob\\n    predictions = Dense(2, activation='softmax')(x)\\n    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\\n    return model\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import SeparableConv2D\n",
    "\n",
    "def xception(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))    \n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2), use_bias=False, name='block1_conv1')(inputs)\n",
    "    x = BatchNormalization(name='block1_conv1_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv1_act')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', use_bias=False, name='block1_conv2')(x)\n",
    "    x = BatchNormalization(name='block1_conv2_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv2_act')(x)\n",
    "\n",
    "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block2_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(256, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block3_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block3_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(728, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block4_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block4_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    for i in range(8):\n",
    "        residual = x\n",
    "        prefix = 'block' + str(i + 5)\n",
    "\n",
    "        x = Activation('relu', name=prefix + '_sepconv1_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv1_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv2_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv2_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv3_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv3_bn')(x)\n",
    "\n",
    "        x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(1024, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block13_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block13_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block13_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv1_act')(x)\n",
    "\n",
    "    x = SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv2_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv2_act')(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "def xception(dropout):\n",
    "    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x) # drop prob\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vanillacnn(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1)) \n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 240, 320, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 120, 160, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 120, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 120, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 40, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 10, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 10, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 10, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 390,786\n",
      "Trainable params: 389,826\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model' + sensor + '.h5'\n",
    "model = vanillacnn(0.5)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "stop = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, stop]\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9080 samples, validate on 2271 samples\n",
      "Epoch 1/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.2828 - sparse_categorical_accuracy: 0.9125Epoch 00000: val_loss improved from inf to 0.28729, saving model to model02.h5\n",
      "9080/9080 [==============================] - 30s - loss: 0.2828 - sparse_categorical_accuracy: 0.9126 - val_loss: 0.2873 - val_sparse_categorical_accuracy: 0.9128\n",
      "Epoch 2/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.1649 - sparse_categorical_accuracy: 0.9404Epoch 00001: val_loss improved from 0.28729 to 0.13692, saving model to model02.h5\n",
      "9080/9080 [==============================] - 29s - loss: 0.1649 - sparse_categorical_accuracy: 0.9404 - val_loss: 0.1369 - val_sparse_categorical_accuracy: 0.9304\n",
      "Epoch 3/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.1073 - sparse_categorical_accuracy: 0.9573Epoch 00002: val_loss improved from 0.13692 to 0.07316, saving model to model02.h5\n",
      "9080/9080 [==============================] - 29s - loss: 0.1070 - sparse_categorical_accuracy: 0.9574 - val_loss: 0.0732 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 4/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0841 - sparse_categorical_accuracy: 0.9700Epoch 00003: val_loss improved from 0.07316 to 0.06189, saving model to model02.h5\n",
      "9080/9080 [==============================] - 29s - loss: 0.0842 - sparse_categorical_accuracy: 0.9698 - val_loss: 0.0619 - val_sparse_categorical_accuracy: 0.9753\n",
      "Epoch 5/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0706 - sparse_categorical_accuracy: 0.9752Epoch 00004: val_loss improved from 0.06189 to 0.06028, saving model to model02.h5\n",
      "9080/9080 [==============================] - 29s - loss: 0.0705 - sparse_categorical_accuracy: 0.9751 - val_loss: 0.0603 - val_sparse_categorical_accuracy: 0.9797\n",
      "Epoch 6/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0634 - sparse_categorical_accuracy: 0.9758Epoch 00005: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0641 - sparse_categorical_accuracy: 0.9756 - val_loss: 0.0701 - val_sparse_categorical_accuracy: 0.9749\n",
      "Epoch 7/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0612 - sparse_categorical_accuracy: 0.9796Epoch 00006: val_loss improved from 0.06028 to 0.05600, saving model to model02.h5\n",
      "9080/9080 [==============================] - 29s - loss: 0.0610 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.0560 - val_sparse_categorical_accuracy: 0.9837\n",
      "Epoch 8/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0558 - sparse_categorical_accuracy: 0.9799Epoch 00007: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0560 - sparse_categorical_accuracy: 0.9798 - val_loss: 0.0669 - val_sparse_categorical_accuracy: 0.9780\n",
      "Epoch 9/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0565 - sparse_categorical_accuracy: 0.9800Epoch 00008: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0565 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.0607 - val_sparse_categorical_accuracy: 0.9824\n",
      "Epoch 10/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0532 - sparse_categorical_accuracy: 0.9813Epoch 00009: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0533 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.0579 - val_sparse_categorical_accuracy: 0.9811\n",
      "Epoch 11/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0453 - sparse_categorical_accuracy: 0.9821Epoch 00010: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0452 - sparse_categorical_accuracy: 0.9822 - val_loss: 0.0671 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 12/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0470 - sparse_categorical_accuracy: 0.9833Epoch 00011: val_loss improved from 0.05600 to 0.05080, saving model to model02.h5\n",
      "9080/9080 [==============================] - 29s - loss: 0.0470 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.0508 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 13/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0455 - sparse_categorical_accuracy: 0.9838Epoch 00012: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0456 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0542 - val_sparse_categorical_accuracy: 0.9850\n",
      "Epoch 14/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0446 - sparse_categorical_accuracy: 0.9847Epoch 00013: val_loss did not improve\n",
      "9080/9080 [==============================] - 28s - loss: 0.0446 - sparse_categorical_accuracy: 0.9846 - val_loss: 0.0510 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 15/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0444 - sparse_categorical_accuracy: 0.9851Epoch 00014: val_loss improved from 0.05080 to 0.05050, saving model to model02.h5\n",
      "9080/9080 [==============================] - 29s - loss: 0.0443 - sparse_categorical_accuracy: 0.9851 - val_loss: 0.0505 - val_sparse_categorical_accuracy: 0.9837\n",
      "Epoch 16/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0392 - sparse_categorical_accuracy: 0.9855Epoch 00015: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0395 - sparse_categorical_accuracy: 0.9855 - val_loss: 0.0540 - val_sparse_categorical_accuracy: 0.9833\n",
      "Epoch 17/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0413 - sparse_categorical_accuracy: 0.9860Epoch 00016: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0414 - sparse_categorical_accuracy: 0.9859 - val_loss: 0.0567 - val_sparse_categorical_accuracy: 0.9833\n",
      "Epoch 18/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0382 - sparse_categorical_accuracy: 0.9860Epoch 00017: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0387 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.0525 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 19/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0391 - sparse_categorical_accuracy: 0.9886Epoch 00018: val_loss improved from 0.05050 to 0.04863, saving model to model02.h5\n",
      "9080/9080 [==============================] - 29s - loss: 0.0392 - sparse_categorical_accuracy: 0.9885 - val_loss: 0.0486 - val_sparse_categorical_accuracy: 0.9859\n",
      "Epoch 20/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0402 - sparse_categorical_accuracy: 0.9864Epoch 00019: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0401 - sparse_categorical_accuracy: 0.9865 - val_loss: 0.0520 - val_sparse_categorical_accuracy: 0.9868\n",
      "Epoch 21/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0360 - sparse_categorical_accuracy: 0.9873Epoch 00020: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0360 - sparse_categorical_accuracy: 0.9873 - val_loss: 0.0535 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 22/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0391 - sparse_categorical_accuracy: 0.9864Epoch 00021: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0390 - sparse_categorical_accuracy: 0.9865 - val_loss: 0.0548 - val_sparse_categorical_accuracy: 0.9859\n",
      "Epoch 23/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0359 - sparse_categorical_accuracy: 0.9883Epoch 00022: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0358 - sparse_categorical_accuracy: 0.9883 - val_loss: 0.0543 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 24/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0362 - sparse_categorical_accuracy: 0.9882Epoch 00023: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0361 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0518 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 25/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0317 - sparse_categorical_accuracy: 0.9901Epoch 00024: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0317 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0548 - val_sparse_categorical_accuracy: 0.9850\n",
      "Epoch 26/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0344 - sparse_categorical_accuracy: 0.9877Epoch 00025: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0343 - sparse_categorical_accuracy: 0.9878 - val_loss: 0.0528 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 27/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0338 - sparse_categorical_accuracy: 0.9886Epoch 00026: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0337 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0539 - val_sparse_categorical_accuracy: 0.9846\n",
      "Epoch 28/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0342 - sparse_categorical_accuracy: 0.9876Epoch 00027: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0341 - sparse_categorical_accuracy: 0.9877 - val_loss: 0.0536 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 29/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0325 - sparse_categorical_accuracy: 0.9898Epoch 00028: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0324 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0497 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 30/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0300 - sparse_categorical_accuracy: 0.9900Epoch 00029: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0299 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.0494 - val_sparse_categorical_accuracy: 0.9846\n",
      "Epoch 31/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0260 - sparse_categorical_accuracy: 0.9915Epoch 00030: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0259 - sparse_categorical_accuracy: 0.9915 - val_loss: 0.0547 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 32/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0293 - sparse_categorical_accuracy: 0.9901Epoch 00031: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0292 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0504 - val_sparse_categorical_accuracy: 0.9846\n",
      "Epoch 33/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0300 - sparse_categorical_accuracy: 0.9891Epoch 00032: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0303 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0543 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 34/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0270 - sparse_categorical_accuracy: 0.9918Epoch 00033: val_loss did not improve\n",
      "9080/9080 [==============================] - 28s - loss: 0.0270 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0565 - val_sparse_categorical_accuracy: 0.9846\n",
      "Epoch 35/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0282 - sparse_categorical_accuracy: 0.9919Epoch 00034: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0281 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.0564 - val_sparse_categorical_accuracy: 0.9850\n",
      "Epoch 36/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0261 - sparse_categorical_accuracy: 0.9907Epoch 00035: val_loss did not improve\n",
      "9080/9080 [==============================] - 28s - loss: 0.0260 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.0520 - val_sparse_categorical_accuracy: 0.9850\n",
      "Epoch 37/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0288 - sparse_categorical_accuracy: 0.9892Epoch 00036: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0288 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.0536 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 38/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0246 - sparse_categorical_accuracy: 0.9915Epoch 00037: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0245 - sparse_categorical_accuracy: 0.9915 - val_loss: 0.0559 - val_sparse_categorical_accuracy: 0.9846\n",
      "Epoch 39/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0318 - sparse_categorical_accuracy: 0.9903Epoch 00038: val_loss improved from 0.04863 to 0.04334, saving model to model02.h5\n",
      "9080/9080 [==============================] - 29s - loss: 0.0317 - sparse_categorical_accuracy: 0.9903 - val_loss: 0.0433 - val_sparse_categorical_accuracy: 0.9859\n",
      "Epoch 40/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0248 - sparse_categorical_accuracy: 0.9916Epoch 00039: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0247 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0509 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 41/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0289 - sparse_categorical_accuracy: 0.9906Epoch 00040: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0292 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.0490 - val_sparse_categorical_accuracy: 0.9859\n",
      "Epoch 42/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0251 - sparse_categorical_accuracy: 0.9902Epoch 00041: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0250 - sparse_categorical_accuracy: 0.9902 - val_loss: 0.0462 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 43/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0253 - sparse_categorical_accuracy: 0.9913Epoch 00042: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0253 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.0525 - val_sparse_categorical_accuracy: 0.9855\n",
      "Epoch 44/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0245 - sparse_categorical_accuracy: 0.9923Epoch 00043: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0244 - sparse_categorical_accuracy: 0.9923 - val_loss: 0.0504 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 45/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0262 - sparse_categorical_accuracy: 0.9904Epoch 00044: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0261 - sparse_categorical_accuracy: 0.9904 - val_loss: 0.0515 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 46/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0239 - sparse_categorical_accuracy: 0.9928Epoch 00045: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0239 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0561 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 47/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0246 - sparse_categorical_accuracy: 0.9923Epoch 00046: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0245 - sparse_categorical_accuracy: 0.9923 - val_loss: 0.0518 - val_sparse_categorical_accuracy: 0.9850\n",
      "Epoch 48/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0218 - sparse_categorical_accuracy: 0.9926Epoch 00047: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0219 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.0575 - val_sparse_categorical_accuracy: 0.9833\n",
      "Epoch 49/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0211 - sparse_categorical_accuracy: 0.9926Epoch 00048: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0211 - sparse_categorical_accuracy: 0.9926 - val_loss: 0.0551 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 50/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0258 - sparse_categorical_accuracy: 0.9916Epoch 00049: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0257 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0542 - val_sparse_categorical_accuracy: 0.9837\n",
      "Epoch 51/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0210 - sparse_categorical_accuracy: 0.9923Epoch 00050: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0209 - sparse_categorical_accuracy: 0.9923 - val_loss: 0.0521 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 52/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0195 - sparse_categorical_accuracy: 0.9926Epoch 00051: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0194 - sparse_categorical_accuracy: 0.9926 - val_loss: 0.0534 - val_sparse_categorical_accuracy: 0.9837\n",
      "Epoch 53/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0218 - sparse_categorical_accuracy: 0.9935Epoch 00052: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0219 - sparse_categorical_accuracy: 0.9934 - val_loss: 0.0516 - val_sparse_categorical_accuracy: 0.9837\n",
      "Epoch 54/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0237 - sparse_categorical_accuracy: 0.9919Epoch 00053: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0236 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.0524 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 55/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0185 - sparse_categorical_accuracy: 0.9928Epoch 00054: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0184 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0538 - val_sparse_categorical_accuracy: 0.9841\n",
      "Epoch 56/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0251 - sparse_categorical_accuracy: 0.9916Epoch 00055: val_loss did not improve\n",
      "9080/9080 [==============================] - 28s - loss: 0.0250 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0534 - val_sparse_categorical_accuracy: 0.9824\n",
      "Epoch 57/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0224 - sparse_categorical_accuracy: 0.9932Epoch 00056: val_loss did not improve\n",
      "9080/9080 [==============================] - 28s - loss: 0.0224 - sparse_categorical_accuracy: 0.9932 - val_loss: 0.0542 - val_sparse_categorical_accuracy: 0.9868\n",
      "Epoch 58/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0196 - sparse_categorical_accuracy: 0.9926Epoch 00057: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0195 - sparse_categorical_accuracy: 0.9926 - val_loss: 0.0533 - val_sparse_categorical_accuracy: 0.9846\n",
      "Epoch 59/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0200 - sparse_categorical_accuracy: 0.9933Epoch 00058: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0199 - sparse_categorical_accuracy: 0.9933 - val_loss: 0.0470 - val_sparse_categorical_accuracy: 0.9833\n",
      "Epoch 60/1000\n",
      "9056/9080 [============================>.] - ETA: 0s - loss: 0.0195 - sparse_categorical_accuracy: 0.9928Epoch 00059: val_loss did not improve\n",
      "9080/9080 [==============================] - 29s - loss: 0.0194 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0556 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 00059: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f65476a02b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          batch_size=32, \n",
    "          epochs=1000, \n",
    "          verbose=1, \n",
    "          callbacks=callbacks_list, \n",
    "          validation_data=(X_val, y_val), \n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'sparse_categorical_accuracy']\n",
      "9056/9080 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.013383198205294706, 0.99570484581497798]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "trained_model = load_model(model_name)\n",
    "print(trained_model.metrics_names)\n",
    "trained_model.evaluate(x=X_train, y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2271/2271 [==============================] - 3s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.043335138333958599, 0.98590929027382679]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.evaluate(x=X_val, y=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
