{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(135109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sensor = '15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    X = []\n",
    "    y = []\n",
    "    data_dir_0 = sorted(os.listdir(os.path.join('pac_data', sensor, '0')))   \n",
    "    i = 0\n",
    "    for f in data_dir_0:\n",
    "        if i % 100 == 0: print(i)\n",
    "        i += 1\n",
    "        X.append(np.load(os.path.join('pac_data', sensor, '0', f))['x'])\n",
    "        y.append(0)\n",
    "    X = np.asarray(X, dtype=np.float16)\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X[indices[:int(0.8*len(indices))]]\n",
    "    y_train = y[indices[:int(0.8*len(indices))]]\n",
    "    X_val = X[indices[int(0.8*len(indices)):]]\n",
    "    y_val = y[indices[int(0.8*len(indices)):]]\n",
    "    labels_present = os.listdir(os.path.join('pac_data', sensor))\n",
    "    if '1' in labels_present:\n",
    "        X = []\n",
    "        y = []\n",
    "        data_dir_1 = sorted(os.listdir(os.path.join('pac_data', sensor, '1')))\n",
    "        i = 0\n",
    "        for f in data_dir_1:\n",
    "            if i % 100 == 0: print(i)\n",
    "            i += 1\n",
    "            X.append(np.load(os.path.join('pac_data', sensor, '1', f))['x'])\n",
    "            y.append(1)        \n",
    "        X = np.asarray(X, dtype=np.float16)\n",
    "        y = np.asarray(y, dtype=int)\n",
    "        indices = np.arange(y.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = np.concatenate([X_train, X[indices[:int(0.8*len(indices))]]])\n",
    "        y_train = np.concatenate([y_train, y[indices[:int(0.8*len(indices))]]])\n",
    "        X_val = np.concatenate([X_val, X[indices[int(0.8*len(indices)):]]])\n",
    "        y_val = np.concatenate([y_val, y[indices[int(0.8*len(indices)):]]])\n",
    "    X_train = np.expand_dims(np.asarray(X_train, dtype=np.float16), axis=3)\n",
    "    y_train = np.asarray(y_train, dtype=int)\n",
    "    X_val = np.expand_dims(np.asarray(X_val, dtype=np.float16), axis=3)\n",
    "    y_val = np.asarray(y_val, dtype=int)\n",
    "    X_train /= 5.\n",
    "    X_val /= 5.\n",
    "    mean_arr = np.mean(X_train, axis = 0)\n",
    "    np.save(os.path.join('pac_data', sensor, 'mean.npy'), mean_arr)\n",
    "    X_train -= mean_arr \n",
    "    X_val -= mean_arr  \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5796, 240, 320, 1) (5796,) (1451, 240, 320, 1) (1451,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling2D, Activation, Dense, Input\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers import concatenate\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import conv_utils\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras.applications.xception import Xception\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#linear\n",
    "def linear(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, x, name='linear')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef xception(dropout):\\n    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\\n    x = base_model.output\\n    x = Flatten()(x)\\n    x = Dropout(dropout)(x) # drop prob\\n    predictions = Dense(2, activation='softmax')(x)\\n    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\\n    return model\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import SeparableConv2D\n",
    "\n",
    "def xception(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1))    \n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2), use_bias=False, name='block1_conv1')(inputs)\n",
    "    x = BatchNormalization(name='block1_conv1_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv1_act')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', use_bias=False, name='block1_conv2')(x)\n",
    "    x = BatchNormalization(name='block1_conv2_bn')(x)\n",
    "    x = Activation('relu', name='block1_conv2_act')(x)\n",
    "\n",
    "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block2_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block2_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(256, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block3_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block3_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block3_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(728, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block4_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block4_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block4_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    for i in range(8):\n",
    "        residual = x\n",
    "        prefix = 'block' + str(i + 5)\n",
    "\n",
    "        x = Activation('relu', name=prefix + '_sepconv1_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv1_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv2_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv2_bn')(x)\n",
    "        x = Activation('relu', name=prefix + '_sepconv3_act')(x)\n",
    "        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')(x)\n",
    "        x = BatchNormalization(name=prefix + '_sepconv3_bn')(x)\n",
    "\n",
    "        x = layers.add([x, residual])\n",
    "\n",
    "    residual = Conv2D(1024, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = Activation('relu', name='block13_sepconv1_act')(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block13_sepconv2_act')(x)\n",
    "    x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block13_sepconv2_bn')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block13_pool')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv1_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv1_act')(x)\n",
    "\n",
    "    x = SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')(x)\n",
    "    x = BatchNormalization(name='block14_sepconv2_bn')(x)\n",
    "    x = Activation('relu', name='block14_sepconv2_act')(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "def xception(dropout):\n",
    "    base_model = Xception(include_top=False, input_shape = (240, 320, 1), pooling='avg')\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x) # drop prob\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions, name='linear')\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vanillacnn(dropout):\n",
    "    inputs = Input(shape=(240, 320, 1)) \n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, x, name='xception')\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 240, 320, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 120, 160, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 120, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 120, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 40, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 10, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 10, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 10, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 390,786\n",
      "Trainable params: 389,826\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model' + sensor + '.h5'\n",
    "model = vanillacnn(0.5)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "stop = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, stop]\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5796 samples, validate on 1451 samples\n",
      "Epoch 1/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0416 - sparse_categorical_accuracy: 0.9908Epoch 00000: val_loss improved from inf to 0.10937, saving model to model15.h5\n",
      "5796/5796 [==============================] - 19s - loss: 0.0416 - sparse_categorical_accuracy: 0.9909 - val_loss: 0.1094 - val_sparse_categorical_accuracy: 0.9959\n",
      "Epoch 2/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0215 - sparse_categorical_accuracy: 0.9964Epoch 00001: val_loss improved from 0.10937 to 0.03578, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0214 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0358 - val_sparse_categorical_accuracy: 0.9959\n",
      "Epoch 3/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0211 - sparse_categorical_accuracy: 0.9965Epoch 00002: val_loss improved from 0.03578 to 0.02679, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0211 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.0268 - val_sparse_categorical_accuracy: 0.9959\n",
      "Epoch 4/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0210 - sparse_categorical_accuracy: 0.9964Epoch 00003: val_loss improved from 0.02679 to 0.02073, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0210 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0207 - val_sparse_categorical_accuracy: 0.9959\n",
      "Epoch 5/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0172 - sparse_categorical_accuracy: 0.9965Epoch 00004: val_loss improved from 0.02073 to 0.01918, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0172 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.0192 - val_sparse_categorical_accuracy: 0.9959\n",
      "Epoch 6/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0155 - sparse_categorical_accuracy: 0.9962Epoch 00005: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0155 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.0200 - val_sparse_categorical_accuracy: 0.9959\n",
      "Epoch 7/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0123 - sparse_categorical_accuracy: 0.9967Epoch 00006: val_loss improved from 0.01918 to 0.01216, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0123 - sparse_categorical_accuracy: 0.9967 - val_loss: 0.0122 - val_sparse_categorical_accuracy: 0.9959\n",
      "Epoch 8/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0162 - sparse_categorical_accuracy: 0.9962Epoch 00007: val_loss improved from 0.01216 to 0.01070, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0162 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.0107 - val_sparse_categorical_accuracy: 0.9959\n",
      "Epoch 9/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0113 - sparse_categorical_accuracy: 0.9965Epoch 00008: val_loss improved from 0.01070 to 0.00879, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0113 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.0088 - val_sparse_categorical_accuracy: 0.9966\n",
      "Epoch 10/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0095 - sparse_categorical_accuracy: 0.9976Epoch 00009: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0095 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0098 - val_sparse_categorical_accuracy: 0.9966\n",
      "Epoch 11/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0104 - sparse_categorical_accuracy: 0.9976Epoch 00010: val_loss improved from 0.00879 to 0.00732, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0104 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0073 - val_sparse_categorical_accuracy: 0.9979\n",
      "Epoch 12/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0084 - sparse_categorical_accuracy: 0.9976Epoch 00011: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0084 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0084 - val_sparse_categorical_accuracy: 0.9972\n",
      "Epoch 13/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0077 - sparse_categorical_accuracy: 0.9978Epoch 00012: val_loss improved from 0.00732 to 0.00666, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0077 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0067 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 14/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0073 - sparse_categorical_accuracy: 0.9981Epoch 00013: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0073 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0094 - val_sparse_categorical_accuracy: 0.9972\n",
      "Epoch 15/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0059 - sparse_categorical_accuracy: 0.9979Epoch 00014: val_loss improved from 0.00666 to 0.00477, saving model to model15.h5\n",
      "5796/5796 [==============================] - 18s - loss: 0.0059 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.0048 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 16/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0085 - sparse_categorical_accuracy: 0.9967Epoch 00015: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0084 - sparse_categorical_accuracy: 0.9967 - val_loss: 0.0156 - val_sparse_categorical_accuracy: 0.9966\n",
      "Epoch 17/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0079 - sparse_categorical_accuracy: 0.9976Epoch 00016: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0078 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0091 - val_sparse_categorical_accuracy: 0.9972\n",
      "Epoch 18/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0031 - sparse_categorical_accuracy: 0.9990Epoch 00017: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0031 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0065 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 19/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0045 - sparse_categorical_accuracy: 0.9981Epoch 00018: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0045 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0064 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 20/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0044 - sparse_categorical_accuracy: 0.9988Epoch 00019: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0044 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0065 - val_sparse_categorical_accuracy: 0.9986\n",
      "Epoch 21/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0020 - sparse_categorical_accuracy: 0.9991Epoch 00020: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0020 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.0063 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 22/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0033 - sparse_categorical_accuracy: 0.9990Epoch 00021: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0033 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 23/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0046 - sparse_categorical_accuracy: 0.9984Epoch 00022: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0046 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.0074 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 24/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0021 - sparse_categorical_accuracy: 0.9991Epoch 00023: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0021 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.0100 - val_sparse_categorical_accuracy: 0.9979\n",
      "Epoch 25/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0026 - sparse_categorical_accuracy: 0.9993Epoch 00024: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0026 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0080 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 26/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0057 - sparse_categorical_accuracy: 0.9979Epoch 00025: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0057 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.0070 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 27/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0031 - sparse_categorical_accuracy: 0.9990Epoch 00026: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0031 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0069 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 28/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0023 - sparse_categorical_accuracy: 0.9991Epoch 00027: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0023 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.0063 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 29/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0017 - sparse_categorical_accuracy: 0.9993Epoch 00028: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0017 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0078 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 30/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0013 - sparse_categorical_accuracy: 0.9995Epoch 00029: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0013 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0090 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 31/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0021 - sparse_categorical_accuracy: 0.9990Epoch 00030: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0021 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0134 - val_sparse_categorical_accuracy: 0.9979\n",
      "Epoch 32/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0046 - sparse_categorical_accuracy: 0.9986Epoch 00031: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0046 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0075 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 33/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0011 - sparse_categorical_accuracy: 0.9998Epoch 00032: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0011 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0085 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 34/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0044 - sparse_categorical_accuracy: 0.9988Epoch 00033: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0044 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0099 - val_sparse_categorical_accuracy: 0.9986\n",
      "Epoch 35/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0068 - sparse_categorical_accuracy: 0.9978Epoch 00034: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0067 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0076 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 36/1000\n",
      "5792/5796 [============================>.] - ETA: 0s - loss: 0.0021 - sparse_categorical_accuracy: 0.9997Epoch 00035: val_loss did not improve\n",
      "5796/5796 [==============================] - 18s - loss: 0.0021 - sparse_categorical_accuracy: 0.9997 - val_loss: 0.0070 - val_sparse_categorical_accuracy: 0.9993\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1a32e23320>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          batch_size=32, \n",
    "          epochs=1000, \n",
    "          verbose=1, \n",
    "          callbacks=callbacks_list, \n",
    "          validation_data=(X_val, y_val), \n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'sparse_categorical_accuracy']\n",
      "5792/5796 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0011682452024102052, 0.99982746721877158]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "trained_model = load_model(model_name)\n",
    "print(trained_model.metrics_names)\n",
    "trained_model.evaluate(x=X_train, y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440/1451 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0047658001054091178, 0.99931082032944396]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.evaluate(x=X_val, y=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
